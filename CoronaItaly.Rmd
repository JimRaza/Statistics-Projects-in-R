---
title: "Modelling of Coronavirus Data from Italy using Count Time Series Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

We present in this report a preliminary study  initiating a three part project into the modelling of  coronavirus data from Italy using count time series models. We will begin the series  with a univariate count time series model applied to the combinaed coronvirus data of all of Italy. We will then proceed to use multivariate models count time series models to several Italian regions, which will allow us to also study regional interactions. We will finish the series with a full spatio-temporal analysis of the data. Our aim in all three studies will be to model and predict the number of new infections.



## Generalized Linear Models for Count Time Series

In the present study we will model the coronavirus data as a univarite count time series of the form:


$$g(\lambda_t) = \beta_0 + \sum_{k=1}^{p} \beta_k \tilde{g}(Y_{t - i_k}) + \sum_{l=1}^q \alpha_l g(\lambda_{t - j_l}) +\mathbf{\eta}^T \mathbf{X}_t$$

Where $\{Y_t: \in \mathbb{N} \}$ denotes the count time series we wish to model.  $\mathbf{X}_t: t \in \mathbb{N}$ denotes an $r$ dimensional covariate vector, $\mathbf{X}_t = ( X_{t,1}, ..., X_{{t,r}} )^T$. We model the conditional mean,$E(Y_t \mid F_{t-1})$,  of the count time series by a process 
$\{\lambda_t : t \in \mathbb{N} \}$ such that $E(Y_t \mid F_{t-1}) = \lambda_t$ . $F_t$ denotes the history of the joint process $\{Y_t, \lambda_t, \mathbf{X}_{t+1} :t \in \mathbb{N} \}$.

$g : R^+ \rightarrow \mathbb{R}$  $\tilde{g}: N_0 \rightarrow \mathbb{R}$ are a link function and transformation function, respectively. The response and the mean values are regressed $p$ and $q$ time steps, respectively. In the language of generalized linear models $\nu_t = g(\lambda_t)$ is called the linear predictor.

We consider two special cases of the above model. If the distriubtion of $Y_t$ is Poisson and both the link and transformation function are identities we obain the INGARCH model:

$$ \lambda_t = \beta_0 + \sum_{k=1}^{p} \beta_k (Y_{t - i_k}) + \sum_{l=1}^q \alpha_l \lambda_{t - j_l} $$
The covariate vector $\mathbf{\eta} = 0$ therefore no covariate effects are included.

If we consider a link function of the form $g(x) = \log (x)$ and transformation function of the form $\tilde{g}(x) = \log(x+1)$ then we obtain the log-linear model:


$$ \nu_t = \log(\lambda_t) = \beta_0 + \sum_{k=1}^{p} \beta_k \log(Y_{t - i_k} + 1) + \sum_{l=1}^q \alpha_l \nu_{t - j_l} $$

We may further divide the above models into categories depending on whether the conditional mean is a Poisson process:$Y_t \mid F_{t-1} \text \sim  \text{Poisson}(\lambda_t)$ or a negative binomial process $Y_t \mid F_{t-1} \text \sim  \text{NegBin}(\lambda_t, \phi)$

$\text{VAR}(Y_t \mid F_{t-1}) = \text{E}(Y_t \mid F_{t-1}) = \lambda_t$ for a conditional Poisson response. For a negative binomial response
$\text{VAR}(Y_t \mid F_{t-1}) = \lambda_t + \lambda_t^{2} / \phi$, where $\phi$ is the dispersion parameter $\phi \in (0, \mathbb{inf})$.

#### Interventions

Interventions are included in the model as covariates and are denoted by $\delta_m$ for the $m$-th intervention.

$$ g(\lambda_t) = \beta_0 + \sum_{k=1}^{p} \beta_k \tilde{g}(Y_{t - i_k}) + \sum_{l=1}^q \alpha_l g(\lambda_{t - j_l}) + \mathbf{ \eta}^T \mathbf{X}_t 
+ \sum_{m=1}^s \omega_m \delta_{m}^{t - \tau_m}    \mathbb{1} (t \geq \tau_m)$$

For a spiky intervention $\delta=0$, for an exponentially decading change in location $\delta \in (0,1)$ and for permanent level shift $\delta=1$.

The model parameters are estimated in the tscount package using the quasi-maximum likelihood approach and BGFS based opmtimization routines.

## Data

Using the dpylr package the data for all the seperate regions of Italy are combined into one set of time series. 

```{r}
library("tidyverse")
library("lubridate")
finalpdf <- read_csv("covid19_italy_region3.csv") %>%
select( Date, HospitalizedPatients:TestsPerformed) %>%
mutate( Date = dmy_hm(Date)) %>%
mutate(Date = date(Date)) %>%
group_by( Date)  %>%
summarize_all(sum)
finalpdf
```

The variable we want to predict is the number of new infections: NewPositiveCases in the dataframe. Plotting the variable against time:
```{r}
finalPos = finalpdf$NewPositiveCases
plot(finalPos)
```

## Results

We observe in the previous plot that the number of of new infections is increasing with time, therefore the data is not stationary. We can deal with this nonstationarity by adding a linear trend as a covariate when we build our model. We condition the response variable on the immediately preceding response variable and  also add a seasonal effect on the mean of 5 days, which is just less than a week,

First we fit an INGARCH mode to the data (with a link function that is the identity). We use the number of people confined to the home as a covariate as well as a linear trend.

```{r}
library("tscount")
regressors <- cbind(finalpdf$HomeConfinement, linearTrend = seq(along = finalPos))
final_pois <- tsglm(finalPos, model = list(past_obs = c(1), past_mean = 5), link= "identity", distr = "poisson", xreg = regressors)
```
This model is not finding any serial dependence in the data. Let us view the autocorrelation function of the residuals:

```{r}
acf(residuals(final_pois), main = "ACF of response residuals")
```

This autocorrelation function confirms that there are strong serial correlations in the residuals and that the model has failed to capture any serial dependence in the original data, In our next attempt we use log link function to give us a loglinar model and view its autocorrelation function:

```{r}

final_pois <- tsglm(finalPos, model = list(past_obs = c(1), past_mean = 5), link= "log", distr = "poisson", xreg = regressors)
```


The autocorrelations of the residuals are now very close to white noise, indicating that the model has succesfuly captured the (linear) serial correlation of the data:


```{r}
acf(residuals(final_pois), main = "ACF of response residuals")
```

We also fit a loglinear model with negative binomial conditional mean to the data:

```{r}

final_nbin <- tsglm(finalPos, model = list(past_obs = c(1), past_mean = 5), link = "log", distr = "nbinom", xreg = regressors)
```

### Model Diagnostics
The tscount package provides us with several tools with which to assess our model fit.

The marginal calibration is the the difference of the average predictive c.d.f. and the emprical c.d.f. of the observations. For an ideal predictive model the marginal calibration should be zero and flat. We can observe in the following graph that while neither models are ideal the marginal calibration of the negative binomial model is significantly better:

```{r}
marcal(final_pois, main = "Marginal calibration")
lines(marcal(final_nbin, plot = FALSE), lty = "dashed")                                  
legend("bottomright", legend = c("Pois", "NegBin"), lwd = 1, lty = c("solid", "dashed"))
```

Our next diagnostic gives us  an indication of the predictive performance of our models. A model whose predictive distribution is identical to the data generating distribution (i.e. an ideal predictive model) would give a Probability Integral Transform (PIT) that is completely flat. Of course with noisy and finite data that ideal is unobtainable in general and we would expect even an ideal model to give distribution that only  approximates a uniform one.

```{r}
pit(final_pois, ylim = c(0, 1.5), main = "PIT Poisson")

```
The PIT for the Poisson model indicates that this model's predictive perfomance is very poor.
```{r}
pit(final_nbin, ylim = c(0, 1.5), main = "PIT Negative Binomial")

```

The PIT for the negative bionomial distribution looks much better but there is still room for improvement. 


```{r}
rbind(Poisson = scoring(final_pois), NegBin = scoring(final_nbin))
```

Another way to assess the predictive performance are proper scoring rules. Various scores are presented above for both models. Most of the proper scores strongly favour the negative binomial model over the Poisson model.


We present a summary of the best model we have found so far:
```{r}
summary(final_pois)

```

The final model our analysis has arrived at has the following form:

$$ \lambda_t = 2.74 + 0.12 Y_{t-1} + 0.323 \lambda_{t-5}  + 0.109t$$ 
The mean has a weak but unmistakable dependence on the respone variable for the previous day. We can also detect an almost weekly seasonal effect. As expected a trend is present. 

### Intervention Analysis

The Italian government ordered a national lockdown  on the 9th of March. It would be interesting to see if we can find evidence of an effect of this lockdown on new infections. The effect of a lockdown is believed to take 7-14 days typically so we cannot pin down an exact date when we expect to observe it. Therefore we use tscounts interv_multiple function to look for the type and timing of the intervention.
```{r}
interv_multiple(final_nbin)

```

No effect of the 9th March lockdown is detected up until the 28th of March. This is an agreement with the generally held view that no flattening behaviour in the number of new infections was observed till this date.
  

## Discussion

We developed a count time series model that gives a reasonable fit to the data, though there is room for improvement. We found that the number of infections depends on the previous days but no days prior that, There was in addition a near-weekly seasonal dependence on the mean.  One avenue for model improvement is the inclusion of a harmonic component with a weekly period as a regressor. No effect from the 9th March lockdown imposed by the Italian government could be detected in the data.



