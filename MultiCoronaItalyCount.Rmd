---
title: "A Multivariate Discrete Count Time-Series Analysis of Corona Infections Data from Italy"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## Introduction

In the previous work I studied the univariate time-series formed from the aggregation of all the new infections time-series from the various regions of Italy.  I extend the work of the previous project to a multivariate discrete count time-series analysis of the same data. The multivariate model fit to the mean of the discrete data is of the following form:

$$
 \mu_{it} = e_{it}\nu_{it} + \lambda_{it} Y_{i, t-1} + \phi_{it} \sum_{j \neq i} w_{ji}Y_{j,t-1}
$$

where  $i = 1, \ldots,I$ l are the various regions of Italy and $t = 1, \ldots, T$ are times. The model has two parts. An endemic part cosisting of the first term and a epidemic part consisting of the remaining two terms.The first term is the endemic part and represent either background infections or infections transmitting from out side of the study region i.e. Italy. The second term models autogressive effects from the previous day and the third term models both spatial and temporal influences on the from different regions at the previous time step. The model also contains an overdispersion parameter $\psi_i > 0 $ and the conditional mean is given by $\mu_{it}(1+ \psi_{it} \mu_{it}) $. The term $e_{it}$ in the endemic part is the expected number of counts.




The model allows for log linear predictors of all three components:

$$
\log(\nu_{it})= \alpha_{i}^{(\phi)} + \beta^{(\phi)T} z_{it^{^\phi}}
$$
$$
\log(\lambda_{it})= \alpha_{i}^{(\nu)} + \beta^{(\nu)T} z_{it^{^\nu}}
$$
$$
\log(\phi_{it})= \alpha_{i}^{(\lambda)} + \beta^{(\lambda)T} z_{it^{^\lambda}}
$$

Finally the $w_{ij}$ variable in the third term reflects the coupling between region $j$ and region $i$. It can be set manually (for instance only allowing infections to arrive from neighbouring regions) or it can be estimated as a parameter of the model.





## The R Package Surveillance

The surveillance package provides the $\tt{hhh4}$ function that fits the above model to count data using a penalized maximum likelihood procedure. The function takes input data in the form of an $\tt{sts}$ object. This $\tt{sts}$ object is formed from three matrices. An $\T \multiply  I$ matirx of observed counts. An $ I by I$ neighbourhood matrix quantifying the coupling between the regions or units. And a matrix of populatlion fractions for each region. The neighbhourhood matrix can generated from a map object of the country and its districts in the form of a SpatialPolygons object.

## Corona Data Preparation

We load the corona data and the tidyverse package for data manipulation, select the columns we need and group the data by region.


```{r}
library("tidyverse")
library("lubridate")

finalpdf <- read_csv("covid19_italy_region3.csv") %>%
select( Date, RegionName, NewPositiveCases) %>%
mutate( Date = dmy_hm(Date)) %>%
mutate(Date = date(Date)) %>%
group_by(RegionName)

finalpdf

```

```{r}
finalpdf2 <- select(finalpdf, Date, RegionName, NewPositiveCases)
#finalpdf <- t(finalpdf)
finalpdf2
```

The hh4 function needs the data in the form of a matrix with regions in the rows and time in the columns
```{r}

wide <- pivot_wider(finalpdf2, names_from = RegionName, values_from = (NewPositiveCases))

```
```{r}
#wide <- mutate(wide, NewProvince = ["P. A. Trento"] + Liguria)
rep <- list(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
wide %>% replace_na( replace=rep)
wide
```

The $\tt{sts}$ object also requires a spatialpolygons representation of the map of Italy and its regions. We load a shape file of Italy and convert it to a spatialpolyons object

```{r}
library(rgdal)
italfullmap<-readOGR(dsn="C:/Users/jamil/Documents/Italy",layer="ITA_adm1",verbose=TRUE)
italmap = subset(italfullmap, NAME_1 != c("Sicily", "Sardegna"))
plot(italmap,col="light green")

```

We want to analyize the Corona data by region. There are 20 regions of italy, as we can see from the spatial polygonds object.
objet

```{r}
mapnames <-italmap@data[,"NAME_1"][1:18]
mapnames
```

However, our corona data for italy has 22 entries


```{r}
cols <- colnames(wide)[1:22]
wide
cols <- sort(cols)
cols

```

We observe that not only does the corona data set seem to have 22 regions instead of 20 the spellings often do not match those from the shape file. On closer inspection we observe that corona dataset has split the infections data for the region of **** has been split into two datsets for its constituents provinces P.A. Bolzano and P. A. Trento. To be consistent with the map data extracted from the shape file this data will need to be recombined to form a colun for the ****** province.

We also need to alter name spellings to match the shape file.

```{r}
wide$`Friuli V.G.`

wide <- rename(wide, P_A_Bol = `P.A. Bolzano`)

wide <- rename(wide, P_A_Tren = `P.A. Trento`)

wide <- rename(wide, Valle_A = `'Valle d''Aosta'`)

#wide <- rename(wide, V_G = `Friuli V.G.`) Does not work for some reason

wide <-select(wide, Date:Veneto)

wide <-select(wide, -Sicilia, -Sardegna)



wide$`P_A_Tren`

wide
```

We also observe that there has been a replication ofthe column for Friuli Venezia Giullia under different names

```{r}
unique(finalpdf$RegionName)
```

Upon examining one of the entires for Frui we see it contains not data, so we will remove it.


```{r}
VG  <- filter(finalpdf, RegionName == "Friuli V. G.")
Venezia <- filter(finalpdf,RegionName == "Friuli Venezia Giulia")

VG


```

Carrying out the combining and other data manipulations

```{r}
wide <- mutate(wide, Trentino_Alto = P_A_Bol + P_A_Tren)
wide <- select(wide,-Date, -P_A_Bol, -P_A_Tren,)
#wide <- replace( is.na(wide), 0)
wide[is.na(wide)] <- 0
cols <- colnames(wide)[1:18]
wide
cols <- sort(cols)
cols

```




```{r}
mapnames <-italmap@data[,"NAME_1"][1:18]
mapnames
```


Renaming the columns of the corona dataframe to match those of the map object
```{r}
wide <- wide %>% rename(Apulia = Puglia)
wide <- wide %>% rename(`Friuli-Venezia Giulia` = `Friuli Venezia Giulia` )
wide <- wide %>% rename(`Trentino-Alto Adige` = `Trentino_Alto`, `Emilia-Romagna` = `Emilia Romagna` )
wide <- wide %>% rename(`Valle d'Aosta` = `Valle_A`)

cols <- colnames(wide)[1:18]
(cols)
```
Now we see the column names match, However, the orders of the columns do not. When we build the data matrix we will have to ensure the column orders match too by arranging them in alphabetical order

```{r}
ready <- wide[1:34,1:18]
#names(ready) <- NULL
ready <- as.matrix(ready)

ready <- ready[, order(colnames(ready))]

```


```{r}
colsfinal <- colnames(ready)
colsfinal
```

There are negative counts in the data which must be mistakes and those are changed to positive values,

```{r}
ready <- ifelse(ready <0 , -ready, ready)

ready 
```

`
```{r}
colnames(ready)
```

For the modelling we need the population fraction in each region Italy. This data is input manually into a list.


```{r}
pop <- list ("Abruzzo" = 1311580, "Apulia"= 4029053 , "Basilicata" = 562869,  "Calabria" = 1947131 , "Campania" = 5801692, "Emilia-Romagna" = 4459477 , "Friuli-Venezia Giulia" = 1215220, "Lazio" = 5879082   ,  "Liguria" = 1550640 ,"Lombardia" = 10060574  ,   "Marche" = 1525271     ,      "Molise" = 305617   ,  "Piemonte" = 4356406   ,  "Toscana" = 3729641 ,"Trentino-Alto Adige" = 1072276   ,     "Umbria" =883015    ,           "Valle d'Aosta" = 125666 ,             "Veneto" = 4905894  )


poptibble <- as_tibble(pop)
poptibble
(colnames(poptibble))
```


We from the time constant matrix of population fractions for each region that is needed to form an $\tt{sts}$ object.


```{r}
popnames <-names(pop)

popmat <- unlist(pop, use.names=FALSE )


sumpop <- sum(popmat)


popmat <- replicate(34, popmat)
popmat <- t(popmat)

italypopfrac <- popmat/sumpop
colnames(italypopfrac) <-  c(popnames)
#italypopfrac <- ready[, order(colnames(italypopfrac))]
(colnames(italypopfrac))
```


Next the matrix of neighbouring adjancies is formed by feeding the spatialpolygons object to the nbOrder function from the spdep library


## Data Modelling and Analysis

We model our data using the following  simplified version of the general model described earlier:

$$
\mu_{it} = e_{i}\nu_{i} + \lambda_{it} Y_{i, t-1} + \phi_{} \sum_{j \neq i} w_{ji}Y_{j,t-1}
$$
The endemic component consist of an intercept, a trend and a periodic component

$$
\log(\nu_t) = \alpha^{(\nu)} + \beta_t t + \gamma \sin(\omega t) + \delta \cos(\omega t)
$$


The $\tt{hhh4}$ modelling function needs a matrix of adjaceny orders for the regions. We can obtain this matrix using the spdep package and inputing a map of Italy as a spatialpolygons object. 

```{r}
library("surveillance") 
library("spdep")




italy_nborder <- nbOrder(poly2adjmat(italmap, zero.policy = TRUE), maxlag =7)


colnames(italy_nborder) <-  c(popnames)
rownames(italy_nborder) <- c(popnames)
```

Next, I  create an R time-series object

```{r}
# create daily time series object
inds <- seq(as.Date("2020-03-01"), as.Date("2020-04-07"), by = "day")

# names need to be manually set for some reason
row.names(italmap) <- (colnames(italypopfrac))
```

From the time series object surveillence time series object ($\tt{sts}$)

```{r}
# finally create sts objecct
italycorona <- sts(observed = ready, start= c(2020, as.numeric(format(inds[1], "%j"))), frequency = 365, neighbourhood = italy_nborder, map = italmap, population = italypopfrac)

```




Next I display the aggregated new infections time series of all Italy (less its Islands) and map indicating diease incidence


```{r} 
plot(italycorona, type = observed ~ time)

popu <- unlist(pop, use.names = TRUE)
popu

italycorona@map$POPULATION <- popu

plot(italycorona, type = observed ~ unit,
  population = italycorona@map$POPULATION/100000,
  labels = list(font = 2), colorkey = list(space = "right"),
  sp.layout = layout.scalebar(italycorona@map, corner = c(0.05, 0.05),
    scale = 50, labels = c("0", "50 km"), height = 0.7))



```
The new infections time series has a trend as a result of the spread of the virus. It can be seen that the North is the most affeced and especially the region of Lombardy. This is almost certainly due to the presence of the major internationa city of Milan which receives many visitors and is a travel hub,



I plot the cout time series for the nine regions with the most infections

```{r}
plot(italycorona, units = which(colSums(observed(italycorona)) > 2000 ))

```


We are now ready to fit a model to our data using the $\tt{hhh4}$ function 

### Model Building with the Surveillance Package

The first fit our basic model which is a fixed effects model where infections can only be passed to next neighbour regions and random model is negative binomial. We assume a peridoic component of weekly period in the endemic component. First the model specification and then the model fit with a Nelder-Mead optimizer.

```{r}
coronaModel_basic <- list(
  end = list(f = addSeason2formula(~1 + t, period = 52), #italycorona@freq),
             offset = population(italycorona)),
  ar = list(f = ~1),
  ne = list(f = ~1, weights = neighbourhood(italycorona) == 1),
  family = "NegBin1")


optimizer <- list(stop = list(tol=1e-5, niter=200),  regression = list(method="CG"), variance = list(method="CG")) #method="Nelder-Mead" & "nlminb"

coronaFit_basic <- hhh4(stsObj = italycorona, control = coronaModel_basic, optimizer )
summary(coronaFit_basic, idx2Exp = TRUE, amplitudeShift = TRUE, maxEV = TRUE)
```

The intercept of the endemic part seems unusually large and with an unusually large error, indicating some problem with the fit. Out of curiosity let's view the periodic part of the endemic fit

```{r}
plot(coronaFit_basic, type = "season", components = "end", main = "")

confint(coronaFit_basic, parm = "overdisp")

```

I fit another model but this time with a Poisson mean and compute the AIC


```{r}
AIC(coronaFit_basic, update(coronaFit_basic, family = "Poisson"))

```
The negative bionomal based model is seen to give a much better fit.

A plot of the contributions of the three components of the model to the mean as a function of time can help us understand the spatio-temporal dynamics of the corona infections

In these plots the endemic compnent is in grey, the autogressive in blue and the spatio-temporal in blue
```{r}
districts2plot <- which(colSums(observed(italycorona)) > 0)
plot(coronaFit_basic, type = "fitted", units = districts2plot, hide0s = TRUE)
```

It seems likely that the covid-19 arrived in Milan from international travellers and then spread to the surrounding regions of Italy. In this case large endemic component would be expected for Lombardi with a small spatiotemporal compenent (because Lombardis is a spreader of infections not a reciever of tbem) and large autoreggressive component from internal transmission. 

This model suggests there were a small number of external infections in Lombardi which rapdily grew threw internal transmissios. A small number of transmission spread to neighbouring regions  of Lombardi and rapdily intreased due to internal transioms. This seems implausible. The extrenal  infections in Lombardi at least should be very much greater to the large number of international travellers arriving there.



### Incorprating Greater Spatial Interaction

I now attempt to model that with more sophisticated models. The first complexity I add is scaling each district's suspetibility $\phi$ by multplying it by  $e_{i}^{\beta_{i}$. 

```{r}
coronaFit_nepop <- update(coronaFit_basic,
  ne = list(f = ~log(pop)), data = list(pop = population(italycorona)))
```

Previously we just assumed a form for the weights that allowed transmission only from neighbouring districts $w_{ij} = {I}(i \sim j)$. Now estimate the weight as parameters from the data, assuming a general form $w_{ij}o_{ji}^{-d}$$j$ where $o_{ji}$ is the adjacncy order between districts $i$ and $j$ and $d$ is a parameter to be estimated. The sum ofthe weights is normalized to one.

```{r}
coronaFit_powerlaw <- update(coronaFit_nepop,
  ne = list(weights = W_powerlaw(maxlag = 5)))

```



```{r}
districts2plot <- which(colSums(observed(italycorona)) > 0)
plot(coronaFit_powerlaw, type = "fitted", units = districts2plot, hide0s = TRUE)
```


```{r}
AIC(coronaFit_nepop, coronaFit_basic, coronaFit_powerlaw)
```

## Random Effects Models

In order to model heterogeniety in the data (and especially in the case of many regions) random effects models can be useful. Here I fit random effects model to the corona data. In such models the intercept of the parameters is modified by adding a random effects term to the a fixed term. The random effect term is assumed to have a normal distribution with zero mean and a variance parameter that is to be eastimated. For instance in the log predictor the intercept for the autogressive term becomes:

$$
c_i \sim \mathcal{N}(0,\,\sigma_{\nu}^{2}) \mathbf{1}
$$


```{r}
coronaFit_ri <- update(coronaFit_powerlaw,
  end = list(f = update(formula(coronaFit_powerlaw)$end, ~. + ri() - 1)),
  ar  = list(f = update(formula(coronaFit_powerlaw)$ar,  ~. + ri() - 1)),
  ne  = list(f = update(formula(coronaFit_powerlaw)$ne,  ~. + ri() - 1)),niter=200, verbose = FALSE )
summary(coronaFit_ri, amplitudeShift = TRUE, maxEV = TRUE)
```


```{r}
head(ranef(coronaFit_ri, tomatrix = TRUE), n = 3)

#stopifnot(ranef(coronaFit_ri) > -1.6, ranef(coronaFit_ri) < 1.6)
for (comp in c("ar", "ne", "end")) {
  print(plot(coronaFit_ri, type = "ri", component = comp,
    col.regions = rev(cm.colors(100)), labels = list(cex = 0.6),
    at = seq(-1.6, 1.6, length.out = 15)))
}


```
```{r}

plot(coronaFit_ri, type = "fitted", units = districts2plot, hide0s = TRUE)

```

```{r}
coronaFit_ri_nepop <- update(coronaFit_nepop,
  end = list(f = update(formula(coronaFit_nepop)$end, ~. + ri() - 1)),
  ar  = list(f = update(formula(coronaFit_nepop)$ar,  ~. + ri() - 1)),
  ne  = list(f = update(formula(coronaFit_nepop)$ne,  ~. + ri() - 1)),niter=200, verbose = FALSE )
summary(coronaFit_ri, amplitudeShift = TRUE, maxEV = TRUE)
```

```{r}
head(ranef(coronaFit_ri_nepop, tomatrix = TRUE), n = 3)

stopifnot(ranef(coronaFit_ri_nepop) > -1.6, ranef(coronaFit_ri_nepop) < 1.6)
for (comp in c("ar", "ne", "end")) {
  print(plot(coronaFit_ri_nepop, type = "ri", component = comp,
    col.regions = rev(cm.colors(100)), labels = list(cex = 0.6),
    at = seq(-1.6, 1.6, length.out = 15)))
}


```



```{r}

plot(coronaFit_ri_nepop, type = "fitted", units = districts2plot, hide0s = TRUE)

```


## Predictive Model Assessment and Validation

Information critera for model selection such as AIC are inapprioprate for random effects models. Therefore I'll use a predictive assessment to judge the competing models.


To get going I use one step ahead prediction on the last week of data. This is equivlanet to so and so and therefore I view this results with caution.
```{r}

tp <- c(27, 33)
models2compare <- paste0("coronaFit_", c("basic", "powerlaw", "ri"))
coronaPreds1 <- lapply(mget(models2compare), oneStepAhead,
  tp = tp, type = "final")
```


I assess these forecasts and hence the models that made them using various scores:
```{r}
SCORES <- c("logs", "rps", "dss", "ses")
coronaScores1 <- lapply(coronaPreds1, scores, which = SCORES, individual = TRUE)
t(sapply(coronaScores1, colMeans, dims = 2))
```

The results are not completely clar. No one of the three model does the best on all of the forecasts. However, the fixed effect power law model's performance is the best overall.  

Now I use genuine one step ahead forecastes for model assesement. This means that the model is refit for every new forecast. This requires greater computational power but will provide a more reliable model assesment.

```{r}
coronaPreds2 <- lapply(mget(models2compare), oneStepAhead,
  tp = tp, type = "rolling", which.start = "final",
  cores = 2 * (.Platform$OS.type == "unix"))
coronaScores2 <- lapply(coronaPreds2, scores, which = SCORES, individual = TRUE)
t(sapply(coronaScores2, colMeans, dims = 2))
```

With a full forecast the results become totally ambigious. It has hard to select one model has performing better than the other two. 



```{r}
set.seed(321)
sapply(SCORES, function (score) permutationTest(
  coronaScores2$coronaFit_ri[, , score],
  coronaScores2$coronaFit_basic[, , score]))
```



In order to further assess the quality of the competing models I apply calibrations tests.

```{r}
calibrationTest(coronaPreds2[["coronaFit_ri"]], which = "rps")
```

```{r}
calibrationTest(coronaPreds2[["coronaFit_basic"]], which = "rps")
```


```{r}
calibrationTest(coronaPreds2[["coronaFit_powerlaw"]], which = "rps")
```



This three test shows we can reject the null hypothesis of a calibrated forcast quite strongly. This is evidence that all the models are unsatisfactory. As a further examination of the callibration of the three model I look at the Probabilty Integral transform (PIT). 


```{r}
par(mfrow = sort(n2mfrow(length(coronaPreds2))), mar = c(4.5, 4.5, 3, 1))
for (m in models2compare)
  pit(coronaPreds2[[m]], plot = list(ylim = c(0, 1.25), main = m))

```
The PIT gives us insights into the deficiencies of the three models. A perfect calibration would be a sample from a uniform distribution. We see that no model attains this. The predictive distributions of the basic model and the powerlaw model have similar PITs. The form of the PIT suggests that the predicive distribution is broader than the actual distribution therefore it lacks in resolution. The Random effects model has PIT that seems to be more even but it suggests a predictive distribution that is slightly biased to the right of the true distribution. On closer inspection this bias can be seen in the other two distribtions as well. To conclude none of the model predictions callibrate well but the callibration of the random effects models is the best.




## Summary and Discussion

In this preliminary study I fitted a multivariate discrete count time-series model to 34 days of Corona new infections data from Italy in order to understand the spread of the virus. The models is implemented in the $\tt{hhh4}$ function of the surveillance R package.  As expected the greatest number of infections are found in Lombardy, a region that contains the international city of Milan, and in the regions surroundings Lombardy. There are far fewer infections in the south of the country. 

The model divides the mean of the count data into two main components - endemic and epidemic. The endemic component is usually attributed to background infections but could also be caused by infections from outside of the study region. In the case of the corona virus the second interpretation of the endemic component seems to be the most apt. The epidemic component of the mean is further divided into two subcomponents. An autoregressive component that respresents new infections from within a region and a spatio-temporal compoment that represents new infections from neighbouring regions.

Within the model class several submodels were fitted in two passes. In the first pass basic fixed effecs models with infections from neighbouring regions and a model that allowed new infections from non neighours. The more general model perfermed better on AIC and BIC model selecion procedures. In the second pass random effects models were fitted, Eventually two models were obtained: a basic fixed effects model with new infections possible from neighouring regions and a random effects model with new infections possible only from neighbouring regions.

The interpretations of the data using these two models are vastly different. The basic model 

AIC and BIC are problematic when used on random effects models therefore both models were compared using predictive validation. The prediction results of the two competing models were ambigious with no model emerging as the winner.


## Conclusion and Future Work

It has not been possible to come to any firm conclusion about the dynamics of corona infections in Italy. TWo models of widely disprate interpreations give simiiar validations. Though the second model seems very unlikely from geopgrahical cosiderations.

I've used only 34 days of data in order to remain consistent with the first study, however, it is neccasry to redo both studies with the larger dataset. This will hopefully give less ambigious results. It may be possible that the structure of the model implemented in the $\tt{hhh4}$ function of the surveillace has inherent limitations and that a better model cannot be found. A limitations of these model seems to be that it does not a allow a periodic or a trend component in the epidemic part, only in the endemic part. A pandemic such as corona in its early stages would probably be best model as a linear trend with periodic effects due to infections caused within a region or in niehgoubrig regios. The exclsion of such effects in the epidemic part forces them to be either background effects or infections from outside of the study regin (Italy in our case). This is a major deficency.

As well as using more data I also plan to apply Granger causlity based methods to further understand the dyamics of corona infections in Italy.



Several mo






